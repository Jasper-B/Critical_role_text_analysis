---
title: "Critical Role"
author: "Jasper Bannenberg"
date: "14/03/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup}
library(tidyverse)
library(tidytext)
library(tidylo)
library(tidymodels)
library(textrecipes)
library(keras)
library(patchwork)
library(ggwordcloud)
library(youtubecaption)
library(rvest)
library(RSelenium)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
theme_set(theme_bw(base_size = 10))
```

### Data scraping

We first need to get the urls for every episode of campaign 2: The Mighty Nein. The urls are scraped with 'youtubecaption' by using a remote drive by 'RSelenium'.

```{r get the playlist information}
# rD <- rsDriver(browser="firefox", port=4545L, verbose=F)
# remDr <- rD[["client"]]
# 
# # navigate to the playlist of campaign 2
# remDr$navigate("https://www.youtube.com/playlist?list=PL1tiwbzkOjQxD0jjAE7PsWoaCrs0EkBH2")
# # accept the terms
# remDr$findElements("class name", "lssxud")[[2]]$clickElement()
# 
# webElem <- remDr$findElement("css", "body")
# webElem$sendKeysToElement(list(key = "end"))
# 
# # get the playlist information
# data <- read_html(remDr$getPageSource()[[1]])
# 
# # get the episode links
# episodes <- 
#   data %>%
#   html_elements("[id='video-title']") %>%
#   html_attr("href") %>%
#   as_tibble() %>%
#   rename(url = value) %>%
#   mutate(episode_title = data %>% html_elements("[id='video-title']") %>% html_text2()) %>%
#   filter(str_detect(episode_title, "Episode")) %>%
#   separate(episode_title, into = c("title", NA, "episode"), sep = "\\|") %>%
#   mutate(episode = str_replace(episode, ".*Episode", "Episode"))
```

Now that we have the links we can download the transcripts.

```{r download transcripts}
# # use the urls for every episode to get the captions
# en_us <- possibly(.f = get_caption, otherwise = NULL)
# 
# temp <- episodes %>%
#   nest(data = c(url)) %>%
#   mutate(text = map(data, ~ en_us(paste0("youtube.com", .x$url)))) 
# 
# # replace episode 102 with en-US subtitles
# replace <- data %>%
#   filter(lengths(text) == 0) %>%
#   nest(data = c(url)) %>%
#   mutate(text = map(data, ~ get_caption(paste0("youtube.com", .x$url), language = "en-US")))
# 
# # !!! youtubecaption does not seem to work anymore
# replace <- 
#   get_caption("https://www.youtube.com/watch?v=hKlo5FWlxnA&list=PL1tiwbzkOjQxD0jjAE7PsWoaCrs0EkBH2&index=104")
# 
# #write_rds(episodes, "./critit_data.RDs")

# read in the saved data of the transcripts
data <- read_rds("./critit_data.RDs")
```

The difficulty with the raw transcripts is that the data is not clean. The episodes start with in-stream commercials and this continues through the ad-break. In order to remove these parts, I will use user generated timestamps for each episodes. The famous youtube user 'Flando' has the timestamps for most episodes, so these comments will be scraped. 

```{r scrape Flando timestamp annotation}
# rD <- rsDriver(browser="firefox", port=4545L, verbose=F)
# remDr <- rD[["client"]]
# 
# # navigate to the playlist of campaign 2
# remDr$navigate("https://www.youtube.com/playlist?list=PL1tiwbzkOjQxD0jjAE7PsWoaCrs0EkBH2")
# # accept the terms
# remDr$findElements("class name", "lssxud")[[2]]$clickElement()
# 
# 
# scrape_stamps <- function(url) {
#   # navigate to video page
#   remDr$navigate(paste0("https://www.youtube.com", url))
#   
#   remDr$findElement("css", "body")$sendKeysToElement(list(key = "end"))
#   
#   # wait until page is loaded
#   Sys.sleep(runif(n = 1, min = 2, max = 3))
#   
#   remDr$findElement("css", "body")$sendKeysToElement(list(key = "end"))
#   
#   # wait until page is loaded
#   Sys.sleep(runif(n = 1, min = 2, max = 3))
#   
#   remDr$findElement("css", "body")$sendKeysToElement(list(key = "end"))
#   
#   # wait until page is loaded
#   Sys.sleep(runif(n = 1, min = 2, max = 3))
#   
#   # get the comments data
#   comments <- read_html(remDr$getPageSource()[[1]])
#   
#   # find number of Flando's comment
#   number <- comments %>%
#     html_elements("[id='author-text']") %>%
#     html_text2() %>%
#     as_tibble() %>%
#     mutate(row = row_number()) %>%
#     filter(value == "Flando Maltrizian") %>%
#     pull(row)
#   
#   # get Flando's comment
#   flando <- comments %>%
#     html_elements("[id='content-text']") %>%
#     .[number] %>%
#     html_children() %>%
#     html_text2() %>%
#     stringi::stri_remove_empty()
#   
#   if (length(number) == 0) {
#     return(NA)
#   }
#   
#   return(flando)
# }
# 
# #write_rds(test, "./flando_timestamps.RDs")
```

### Data Cleaning

We can obtain the clean data by removing the intro and breaks with Flando's timestamps

```{r clean data with timestamps}
# read in flando's data
flando <- read_rds("./flando_timestamps.RDs")

# here I will pull all available data where Flando annotated episode and break starts
start_times <-
  flando %>%
  unnest(flando) %>%
  drop_na() %>%
  mutate(id = row_number()) %>%
  filter(grepl("episode starts", flando, ignore.case = T) |
         grepl("break starts", flando, ignore.case = T) |
         grepl("break ends", flando, ignore.case = T)) %>%
  pull(id) # get the row number of every episode and break start

# I will filter only episodes with 3 values (that means that all 3 timestamps were annotated by Flando)
episodes <-
  flando %>%
  unnest(flando) %>%
  drop_na() %>%
  mutate(id = row_number()) %>%
  filter(grepl("episode starts", flando, ignore.case = T) |
         grepl("break starts", flando, ignore.case = T) |
         grepl("break ends", flando, ignore.case = T)) %>%
  group_by(episode) %>%
  count() %>%
  filter(n == 3) %>%
  ungroup() %>%
  pull(episode)

# here I will filter all episodes where we have correct annotation and convert time to minutes
timestamps <-
  flando %>%
  unnest(flando) %>%
  drop_na() %>%
  mutate(id = row_number()) %>%
  filter(episode %in% episodes & id %in% (start_times - 1)) %>% # the start time is in the row above the annotation, thus rownumber - 1
  separate(flando, into = c("h", "m", "s"), sep = ":", fill = "left") %>% # convert to minutes
  mutate(h = as.numeric(h),
         h = if_else(is.na(h), 0, h),
         m = as.numeric(m),
         s = as.numeric(s),
         time = h*60 + m +(s/60)) %>%
  select(-h, -m, -s)

# here I unnest all data to get the subtitles for each episode and convert the time to minutes
unnested_data <- 
  data %>% 
  unnest(text) %>%
  mutate(start = start / 60)

# make a list to put the clean data into
timestamp_data <- list()

# this loop will take the timestamps and only keep the text between the timestamps
for (i in unique(timestamps$title)) {
  temp <- # pull the timestamps
    timestamps %>%
    filter(title == i) %>%
    pull(time)
  
  clean <-
    unnested_data %>%
    filter(title == i) %>%
    mutate(breaks = case_when(start >= temp[1] & start <= temp[2] ~ "yes",
                            start >= temp[3] ~ "yes",
                            TRUE ~ "no")) %>% # search for text (roughly) between the timestamps 
    filter(breaks == "yes") %>%
    select(-breaks)
  
  timestamp_data[[i]] <- clean # add to list
}

# make the list into a dataframe with the cleaned data
timestamp_data <- do.call(bind_rows, timestamp_data)
```

Using the time stamps, we have 83 episodes left over with cleaned data which we will continue with.

Let's look at the results and count how many lines each speaker speaks

```{r clean data with timestamps}
# let's create a helper function to clean the data
clean_data <- function(data) {
  data %>%
    select(episode, segment_id, text) %>%
  group_by(episode) %>%
  mutate(speaker = if_else(str_extract(text, "[A-Z]*:") %>% nchar() > 1,
                           str_extract(text, "[A-Z]*:"), ""), # speaker is everything before ':'
         speaker = na_if(speaker, ""), # if ':' appears without speaker, make NA
         pos = 1:n(),
         speaker = if_else(pos == 1, "MATT:", speaker)) %>% # replace every first text for each episode by MATT
  select(-pos) %>%
  ungroup() %>%
  mutate(text = str_remove(text, "[A-Z]*:"), # remove speaker from text
         text = str_replace_all(text, "\n", " "), # replace \n by space
         speaker = if_else(speaker == "", "NA", speaker),
         speaker = str_remove(speaker, ":"), # remove ':' from speaker
         filter = ifelse(!is.na(speaker), row_number(), NA), # add a filter for use later to merge text per speaker
         speaker = zoo::na.locf(speaker), # replace each NA with the value before it
         filter = zoo::na.locf(filter))
}


# first we need to look at the speakers that the data consists of
clean_data <-
  timestamp_data %>%
  clean_data()
  
# I will remove all the misspelled names and keep only the top 10
clean_data %>%
  count(speaker) %>%
  ggplot(aes(n, fct_reorder(speaker, n))) +
  geom_col() +
  scale_x_continuous(expand = c(0,0)) +
  labs(title = "Amount of lines per speaker",
       y = "Speaker")

# extract only the top players
top_names <-
  clean_data %>% 
              count(speaker) %>%
              slice_max(n = 8, order_by = n) %>%
              pull(speaker)

# keep only the top 8 speakers
clean_data <- 
  clean_data %>%
  filter(speaker %in% top_names)

# most lines are spoken by Matt, which is logical seeing as he is the dungeon master
clean_data %>%
  count(speaker) %>%
  ggplot(aes(x = n, y = fct_reorder(speaker, n))) +
  geom_col(show.legend = F) +
  scale_x_continuous(expand = c(0,0), 
                     limits = c(0, 180000)) +
  labs(title = "Number of lines spoken per player",
       y = NULL) +
  theme(plot.title = element_text(hjust = .5))

#ggsave("./Lines_spoken_per_speaker.jpg", device = "jpeg", width = 10, height = 10)

```

### Data Analysis

Now I can analyse which words are spoken most often by each player. For this we unnest the text into words, remove stopwords, and use log term-frequency to get the words that are most able to distinguish between each player. I will plot both by the absolute frequency as the log odds.

```{r words spoken by each player}
# We can get the log odds for each speaker to determine their most used words
crit_lo <-
  clean_data %>%
  unnest_tokens(word, text, token = "words") %>% # unnest the text into word-level
  anti_join(stop_words) %>% # remove common stopwords
  anti_join(tibble(word = c("yeah", tolower(top_names))), 
            by = "word") %>% # remove specific stopwords that I added (such as their names)
  count(speaker, word) %>%
  bind_log_odds(speaker, word, n) %>% # the log odds gives a measure to compare between players
  arrange(-log_odds_weighted)

# by log odds
crit_lo %>%
  group_by(speaker) %>%
  top_n(n = 15) %>% # keep only the top 15 words per player
  ungroup() %>%
  mutate(word = reorder_within(word, log_odds_weighted, n)) %>% # reorder within groups
  ggplot(aes(log_odds_weighted, word, fill = speaker)) +
  geom_col(show.legend = F, width = .9, alpha = .75) +
  scale_y_reordered(expand = c(0,0)) +
  scale_x_continuous(expand = c(0,0), breaks = scales::pretty_breaks()) +
  scale_fill_viridis_d() +
  facet_wrap(~speaker, scales = "free") +
  labs(title = "Which words are most often spoken by which player?",
       y = NULL,
       x = "Weighted log odds")

#ggsave("./Words_spoken_by_player_log_odds.pdf", height = 9, width = 10)

# only by number of times spoken
crit_lo %>%
  group_by(speaker) %>%
  slice_max(n = 15, order_by = n) %>% # keep only the top 15 words per player
  ungroup() %>%
  mutate(word = reorder_within(word, n, n)) %>% # reorder within groups
  ggplot(aes(n, word, fill = speaker)) +
  geom_col(show.legend = F, width = .9, alpha = .75) +
  scale_y_reordered(expand = c(0,0)) +
  scale_x_continuous(expand = c(0,0), breaks = scales::pretty_breaks()) +
  scale_fill_viridis_d() +
  facet_wrap(~speaker, scales = "free") +
  labs(title = "Which words are most often spoken by which player?",
       y = NULL,
       x = "Number of times spoken")

#ggsave("./Words_spoken_by_player_n.pdf", height = 9, width = 10)
```

A more clear way to represent the most-spoken words is by making wordclouds for each player. For this I will use 'ggwordcloud'.

```{r wordclouds by frequency}
# helper function for wordcloud generation by frequency
wordcloud_n <- function(data = crit_lo, filter) {
  temp <- data %>%
    filter(speaker == filter)
  
  temp %>%
    mutate(angle = 90 * sample(c(0, 1), n(), replace = TRUE, prob = c(60, 40))) %>%
    slice_max(n = 25, order_by = n) %>%
    ggplot(aes(label = word, size = n, color = n, angle = angle)) +
    geom_text_wordcloud_area(shape = "circle") +
    scale_size_area(max_size = 14) +
    theme_minimal() +
    scale_color_viridis_c(option = "C") +
    labs(title = filter)
}

# plot the wordclouds by frequency
set.seed(1234)
wordcloud_n(filter = unique(crit_lo$speaker)[1]) + 
  wordcloud_n(filter = unique(crit_lo$speaker)[2]) +
  wordcloud_n(filter = unique(crit_lo$speaker)[3]) +
  wordcloud_n(filter = unique(crit_lo$speaker)[4]) +
  wordcloud_n(filter = unique(crit_lo$speaker)[5]) +
  wordcloud_n(filter = unique(crit_lo$speaker)[6]) +
  wordcloud_n(filter = unique(crit_lo$speaker)[7]) +
  wordcloud_n(filter = unique(crit_lo$speaker)[8]) +
  plot_layout(ncol = 4) +
  plot_annotation(title = 'Which words are spoken most by each player?',
                  theme = theme(plot.title = element_text(size = 28),
                                plot.background = element_rect(fill = 'gray70', 
                                                               color = "gray70"))) & 
  theme(plot.title = element_text(hjust = 0.5))

#ggsave("./Wordcloud_n.pdf", height = 9, width = 19)
#ggsave("./Wordcloud_n.jpg", height = 9, width = 19, device = "jpeg")
```

And repeat for the log odds

``` {r wordcloud by log odds}
# helper function for wordcloud generationby log odds
wordcloud_log <- function(data = crit_lo, filter) {
  temp <- data %>%
    filter(speaker == filter)
  
  temp %>%
    mutate(angle = 90 * sample(c(0, 1), n(), replace = TRUE, prob = c(70, 30))) %>%
    slice_max(n = 20, order_by = log_odds_weighted) %>%
    ggplot(aes(label = word, size = log_odds_weighted, color = log_odds_weighted, angle = angle)) +
    geom_text_wordcloud_area(shape = "circle", rm_outside = T) +
    scale_size_area(max_size = 12) +
    theme_minimal() +
    scale_color_viridis_c(option = "C") +
    labs(title = filter) +
    theme(plot.title = element_text(hjust = 0.5))
}

# plot the wordclouds by log odds
set.seed(1234)
wordcloud_log(filter = unique(crit_lo$speaker)[1]) + 
  wordcloud_log(filter = unique(crit_lo$speaker)[2]) +
  wordcloud_log(filter = unique(crit_lo$speaker)[3]) +
  wordcloud_log(filter = unique(crit_lo$speaker)[4]) +
  wordcloud_log(filter = unique(crit_lo$speaker)[5]) +
  wordcloud_log(filter = unique(crit_lo$speaker)[6]) +
  wordcloud_log(filter = unique(crit_lo$speaker)[7]) +
  wordcloud_log(filter = unique(crit_lo$speaker)[8]) +
  plot_layout(ncol = 4) +
  plot_annotation(title = 'Which words are spoken most by each player?',
                  theme = theme(plot.title = element_text(size = 28),
                                plot.background = element_rect(fill = 'gray70', color = "gray70"))) & 
  theme(plot.title = element_text(hjust = 0.5)) 

#ggsave("./Wordcloud_log_odds.pdf", height = 9, width = 19, device = "pdf")
#ggsave("./Wordcloud_log_odds.jpg", height = 9, width = 19, device = "jpeg")
```

We can also try to scrape from the whole dataset every time a player speaks a number. I will use this as an estimate for the average amount each player rolls. This is quite a stretch because players can use numbers for other things as well, but I think on average this should cancel out. 

```{r dice rolls}
# clean all data
number_data <-
  unnested_data %>%
  clean_data() %>%
  filter(speaker %in% top_names) %>%
  select(episode, text, speaker) %>%
  unnest_tokens(word, text, token = "words") %>% # unnest the text into word-level
  anti_join(stop_words) %>% # remove common stopwords
  anti_join(tibble(word = c("yeah", tolower(top_names), "ch")), 
            by = "word") %>% # remove specific stopwords that I added (such as their names)
  count(episode, speaker, word)

# plot the average roll with standard deviation
number_data %>%
  mutate(speaker = str_to_sentence(speaker)) %>%
  filter(!is.na(as.integer(word)) & speaker != "Matt") %>%
  mutate(number = parse_number(word),
         number = as.integer(number)) %>%
  filter(number <= 30) %>%
  group_by(speaker) %>%
  summarise(mean = mean(number),
            sd = sd(number)) %>%
  ggplot(aes(speaker, mean)) +
  geom_col(alpha = .7, fill = "midnightblue") +
  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), 
                width = .2, size = 1, col = "midnightblue") +
  scale_y_continuous(expand = c(0,0), limits = c(0, 30)) +
  labs(x = NULL, y = "Average roll",
       "Average rolls across the whole campaign")

#ggsave("./Average_rolls.jpg", width = 8, height = 5, device = "jpeg")
```

We can see that the rolls are quite evenly distributed (as you would expect), but that Sam has on average the highest rolls (probably because rogues have higher modifiers and thus on average higher rolls).

### Machine Learning

Let's try to see if we can use a model to distinguish between the dungeon master Matt and the players. For this, we will group the text together for each speaker, so that we do not have to predict every separate line. I will try to use a deep learning method known as "long short-term memory".

```{r prepare for ML}
# prepare the clean data for machine learning
ml_data <-
  clean_data %>%
  group_by(episode, speaker, filter) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  arrange(filter) %>%
  filter(str_count(text, " ") >= 8) %>% # filter string with lower than 8 words
  ungroup() %>%
  mutate(speaker = if_else(speaker == "MATT", 0, 1),
         speaker = as.integer(speaker)) # need numeric, 0 is DM and 1 is a player

# split the data and keep the same ratio of speakers
set.seed(123)
critit_split <- initial_split(ml_data, strata = speaker) 
critit_train <- training(critit_split) # training data
critit_test <- testing(critit_split) # testing data
```

To preprocess the data, the text will be tokenized into words, and then the top of these words is one-hot encoded into numeric data.

```{r preprocess}
# we will extract words as tokens and onehot encode for use in the LSTM model
critit_rec <-
  recipe(~ text, data = critit_train) %>%
  step_tokenize(text) %>%
  step_tokenfilter(text) %>%
  step_sequence_onehot(text, sequence_length = 100)

# is the sequence_length chosen well?
ml_data %>% 
  mutate(n_words = str_count(text),
         speaker = factor(speaker)) %>%
  ggplot(aes(n_words, fill = speaker)) +
  geom_histogram(binwidth = 5) +
  scale_x_continuous(limits = c(0, 500), expand = c(0,0))
# the average amount of words per text bubble seems to be between 50-100, we will choose 100

# we can use a helper function to make the data ready for deep learning
to_matrix <- function(data) {
  critit_rec %>%
    prep() %>% # prepare the recipe
    bake(new_data = data, composition = "matrix") # bake the recipe and return a matrix
}

# keras needs input as a matrix
critit_train_prep <-
  to_matrix(data = NULL)
```

```{r validation set}
# we can also split the training set in a validation and assessment set to have more control
# over the validation
critit_val <- validation_split(critit_train, strata = speaker)

# analysis set
critit_analysis <-
  to_matrix(data = analysis(critit_val$splits[[1]]))

# assessment set
critit_assess <-
  to_matrix(data = assessment(critit_val$splits[[1]]))

# get the classes
state_analysis <-
  analysis(critit_val$splits[[1]]) %>% pull(speaker)
state_assess <-
  assessment(critit_val$splits[[1]]) %>% pull(speaker)
```

Now we can start the keras model

```{r fit model}
# we start the lstm model via keras
lstm_mod <- keras_model_sequential() %>%
  layer_embedding(input_dim = nrow(critit_train_prep), output_dim = 32) %>% 
  layer_lstm(units = 32) %>%
  layer_dense(units = 1, activation = "sigmoid")
lstm_mod

# use the default compilers
lstm_mod %>%
  compile(optimizer = "adam",
          loss = "binary_crossentropy",
          metrics = c("accuracy")
  )

# fit the model on the training data
lstm_fit <-
  lstm_mod %>%
  keras::fit(
    critit_analysis,
    state_analysis,
    epochs = 10,
    validation_data = list(critit_assess, state_assess),
    batch_size = 512
  )

plot(lstm_fit)
```

It reaches an accuracy of .8104 on the training set, let's see how it fares on the test data that the model has not seen before:

```{r assess on test data}
# convert the test data to matrix format
critit_test_prep <-
  to_matrix(data = critit_test)

# get the final accuracy
accuracy <- lstm_mod %>%
  evaluate(critit_test_prep, critit_test %>% pull(speaker))
accuracy
# the final accuracy is .809 so no overfitting!

# get the predictions
test_pred <-
  lstm_mod %>% 
  predict(critit_test_prep) %>% 
  `>`(0.5) %>% 
  k_cast("int32") %>%
  as.integer() %>%
  cbind(critit_test, .) %>%
  as_tibble() %>%
  rename(pred_speaker = ".") %>%
  mutate(speaker = factor(speaker),
         pred_speaker = factor(pred_speaker))

# confusion matrix
test_pred %>%
  mutate(speaker = if_else(speaker == 0, "DM", "Player"),
         pred_speaker = if_else(pred_speaker == 0, "DM", "Player")) %>%
  conf_mat(truth = speaker, estimate = pred_speaker) %>%
  autoplot(type = "heatmap")
```

As a final part, I will test the model on the complete 142 uncleaned dataset of episodes of campaign 2.

```{r predict on new data}
# clean the complete dataset
complete_data <-
  unnested_data %>%
  clean_data() %>%
  filter(speaker %in% top_names) %>%
  group_by(episode, speaker, filter) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  arrange(filter) %>%
  filter(str_count(text, " ") >= 8) %>% # filter string with lower than 8 words
  ungroup() %>%
  mutate(speaker = if_else(speaker == "MATT", 0, 1),
         speaker = as.integer(speaker))

# convert to matrix
complete_data_prep <-
  to_matrix(data = complete_data)

# get the final accuracy
accuracy_complete <- lstm_mod %>%
  evaluate(complete_data_prep, complete_data %>% pull(speaker))
accuracy_complete
# the model works similarly on the complete data! Quite a robust model

# get the predictions
test_pred_complete <-
  lstm_mod %>% 
  predict(complete_data_prep) %>% 
  `>`(0.5) %>% 
  k_cast("int32") %>%
  as.integer() %>%
  cbind(complete_data, .) %>%
  as_tibble() %>%
  rename(pred_speaker = ".") %>%
  mutate(speaker = factor(speaker),
         pred_speaker = factor(pred_speaker))

# confusion matrix
test_pred_complete %>%
  mutate(speaker = if_else(speaker == 0, "DM", "Player"),
         pred_speaker = if_else(pred_speaker == 0, "DM", "Player")) %>%
  conf_mat(truth = speaker, estimate = pred_speaker) %>%
  autoplot(type = "heatmap")

#ggsave("./Final_prediction.jpg")

```

We reach a final accuracy of .80 to predict whether text is spoken by the dungeon master of by a player. From the confusion matrix we can see that the model is very accurate in recognizing when a player is speaking, but it has trouble to distinguish when the dungeon master is speaking. This could be due to class imbalance with more instances of players speaking than the DM speaking. However, for deep learning it is not wise to downsample as the amount of datapoints is very important. An option could be to split the text of the DM into more separate datapoints. I will stop here though, as the learning objective of this project was not to optimize the model but to get more familiar with text-based deep learning models.